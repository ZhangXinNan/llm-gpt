
- 序章 看似寻常最奇崛，成如容易却艰辛
  - 0.1 GPT-4：点亮人工智能的火花
  - 0.2 人工智能演进之路：神经网络两落三起
  - 0.3 现代自然语言处理：从规则到统计
    - 0.3.1 何为语言？信息又如何传播？
    - 0.3.2 NLP是人类和计算机沟通的桥梁
    - 0.3.3 NLP技术的演化史
  - 0.4 大规模预训练模型：BERT与GPT争锋
    - 0.4.1 语言模型的诞生和进化
    - 0.4.2 统计语言模型的发展历程
    - 0.4.3 基于Transformer架构的预训练模型
    - 0.4.5 “预训练+微调”的模式
    - 0.4.6 以提示、指令模式直接使用大模型
  - 0.5 从初代GPT到ChatGPT，再到GPT-4
    - 0.5.1 GPT作为生成式模型的天然优势
    - 0.5.2 ChatGPT背后的推手——OPENAI
    - 0.5.3 从初始代GPT到ChatGPT，再到GPT-4的进化史


序章 看似寻常最奇崛，成如容易却艰辛
______
## 0.1 GPT-4：点亮人工智能的火花
大规模语言模型（Language-scale Language Model, LLM，也称为大模型）

人工通用智能（Artificial General Intelligence， AGI）

## 0.2 人工智能演进之路：神经网络两落三起
人工智能有两大核心应用：计算机视觉（Computer Vision, CV）和自然语言处理（NLP）

## 0.3 现代自然语言处理：从规则到统计
### 0.3.1 何为语言？信息又如何传播？
### 0.3.2 NLP是人类和计算机沟通的桥梁
### 0.3.3 NLP技术的演化史
* 起源：图灵测试
* 基于规则：基于语法和语义规则
* 基于统计：语言模型（Language Model）捕捉自然语言中词汇、短语和句子的概率分布的统计模型。
* 深度学习和大数据驱动：

## 0.4 大规模预训练模型：BERT与GPT争锋
### 0.4.1 语言模型的诞生和进化
* 条件概率公式
* 马尔可夫假设

### 0.4.2 统计语言模型的发展历程
* 1948年，N-Gram模型，基于前N-1个词来预测序列中的第N个词
* 1954年，Bag-of-Words模型：将文本表示为一个单词的集合，而不考虑单词在文本中的顺序。
* 1986年，分布式表示（Distributed Representation）：词或短语表示为数值向量。
* 2003年，神经概率语言模型
* 2013年，Word2Vec：通过训练神经网络来学习词汇的分布式表示。
  * CBOW(Continuous Bag of Words)词续词袋：预测一个单词的上下文来学习词向量
  * Skip-Gram模型：预测目标单词周围的单词来学习词向量。
* 2018年之后，基于Transformer的预训练语言模型一统江湖。

### 0.4.3 基于Transformer架构的预训练模型
自然语言处理中的预训练，指在大量无标注文本数据上训练语言模型。

### 0.4.5 “预训练+微调大模型”的模式

### 0.4.6 以提示、指令模式直接使用大模型
提示prompt：输入一个词或者短语，根据这个提示生成自然且连贯的文本。
指令Instruct：输入是一条明确的指令，要求模型完成特定任务。

## 0.5 从初代GPT到ChatGPT，再到GPT-4
GPT（Generative Pre-Training）：利用Transformer模型对大量文本进行无监督学习，其目标就是最大化语句序列出现的概率。

### 0.5.1 GPT作为生成式模型的天然优势
BERT更像是填空题，GPT更像是文字接龙游戏。GPT更接近语言模型的本质，因为它的预训练过程紧凑且有效地再现了自然语言生成的过程。

### 0.5.2 ChatGPT背后的推手——OPENAI

### 0.5.3 从初始代GPT到ChatGPT，再到GPT-4的进化史