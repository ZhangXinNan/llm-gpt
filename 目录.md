
- 序章 看似寻常最奇崛，成如容易却艰辛
  - 0.1 GPT-4：点亮人工智能的火花
  - 0.2 人工智能演进之路：神经网络两落三起
  - 0.3 现代自然语言处理：从规则到统计
    - 0.3.1 何为语言？信息又如何传播？
    - 0.3.2 NLP是人类和计算机沟通的桥梁
    - 0.3.3 NLP技术的演化史
  - 0.4 大规模预训练模型：BERT与GPT争锋
    - 0.4.1 语言模型的诞生和进化
    - 0.4.2 统计语言模型的发展历程
    - 0.4.3 基于Transformer架构的预训练模型
    - 0.4.5 “预训练+微调”的模式
    - 0.4.6 以提示、指令模式直接使用大模型
  - 0.5 从初代GPT到ChatGPT，再到GPT-4
    - 0.5.1 GPT作为生成式模型的天然优势
    - 0.5.2 ChatGPT背后的推手——OPENAI
    - 0.5.3 从初始代GPT到ChatGPT，再到GPT-4的进化史
- 第1课 语言模型的雏形N-Gram和简单文本表示Bag-of-Words
  - 1.1 N-Gram 模型
  - 1.2 “词”是什么，如何“分词”
  - 1.3 创建一个BiGram字符预测模型
  - 1.4 词袋模型
  - 1.5 用词袋模型计算文本相似度
- 第2课 词的向量表示Word2Vec和Embedding
  - 2.1 词向量约等于词嵌入
  - 2.2 Word2Vec：CBOW模型和Skip-Gram模型
  - 2.3 CBOW模型的代码实现
  - 2.4 通过nn.Embedding来实现词嵌入
- 第3课 神经概率语言模型和循环神经网络
  - 3.1 NPLM的起源
  - 3.2 NPLM的实现
  - 3.3 循环神经网络的实现
  - 3.4 循环神经网络实现
- 第4课 Seq2Seq 编码器-解码器架构
- 第5课 引入注意力机制
- 第6课 搭建GPT核心组件Transformer
- 第7课 训练出你的简版GPT
- 第8课 ChatGPT 基于人类反馈的强化学习
- 第9课 使用强大的GPT-4 API

